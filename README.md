# cohen-linalg


Notes and code experiments for linear algebra done SVD. The idea is to construct the SVD as soon as possible, then use it for everything else that follow &mdash; from characterizing invertbility to parametrizing the loss surface of linear regression with linearly dependent data columns. Some interesting stuff that is covered:
  * Proof of the real spectral theorem, and a code demo.
  * Proof of the singular value decomposition (SVD). 
  * Characterizing the loss surface of a linear regression problem.
  * An extensive discussion of the Moore-Penrose pseudoinverse.

The selection and progression of topics follow the course [Complete linear algebra: theory and implementation in code](https://www.udemy.com/course/linear-algebra-theory-and-implementation/) by [Prof. Mike X Cohen](http://mikexcohen.com/). But all writing in this repo (code and prose) and the errors that come with it are my own. Check the course out, it's the best course out there for people interested in doing machine learning.

<br>

## Quick links

* [Proofs involving the Moore-Penrose pseudoinverse](https://en.wikipedia.org/wiki/Proofs_involving_the_Moore%E2%80%93Penrose_inverse)
* [KaTeX Supported Functions](https://katex.org/docs/supported.html)

<br>

## References
* [Sheldon Axler. *Down With Determinants!*. The American Monthly. (1996)](https://www.maa.org/sites/default/files/pdf/awards/Axler-Ford-1996.pdf)
* [Leslie Hogben (editor), *Handbook of Linear Algebra*. CRC Press 2014.](https://www.oreilly.com/library/view/handbook-of-linear/9781466507296/)
* [Cleve Moler. *Numerical Computing with MATLAB*. The MathWorks / SIAM. (2013)](https://www.mathworks.com/moler/index_ncm.html)
* [Peter Olver and Chehzrad Shakiban. *Applied Linear Algebra*. UTM Springer. (2018)](https://www-users.math.umn.edu/~olver/books.html)
* [Petersen & Pedersen. *The Matrix Cookbook*. v. Nov. 15, 2012.](https://www.math.uwaterloo.ca/~hwolkowi/matrixcookbook.pdf)
